{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'ignore', 'over': 'warn', 'under': 'ignore', 'invalid': 'ignore'}"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "from math import exp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.datasets import load_boston\n",
    "from scipy.spatial import distance\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import PP4\n",
    "from sklearn.metrics import f1_score\n",
    "import copy\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Letter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: read_table is deprecated, use read_csv instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter :  0\n",
      "iter :  1\n",
      "iter :  2\n",
      "iter :  3\n",
      "iter :  4\n",
      "iter :  5\n",
      "iter :  6\n",
      "iter :  7\n",
      "iter :  8\n",
      "iter :  9\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_table('letter-recognition.data', sep=',', header=None)\n",
    "X_data = df.drop(0, axis=1)\n",
    "scaler = MinMaxScaler(feature_range =(-1,1))\n",
    "scaler.fit(X_data)\n",
    "scaled = pd.DataFrame(scaler.transform(X_data))\n",
    "scaled['class'] = df[0]\n",
    "df = scaled\n",
    "classes = df['class'].unique()\n",
    "\n",
    "normal = ['T', 'I', 'D', 'N', 'G', 'S', 'B', 'A', 'J', 'M', 'X', 'O', 'R']\n",
    "out = ['F', 'C', 'H', 'W', 'L', 'P', 'E', 'V', 'Y', 'Q', 'U', 'K', 'Z']\n",
    "indice = []\n",
    "outIndice = []\n",
    "for letter in normal:\n",
    "    indice += df['class'].index[df['class'] == letter].tolist()\n",
    "\n",
    "for letter in out:\n",
    "    outIndice += df['class'].index[df['class'] == letter].tolist()\n",
    "indice = np.sort(indice)\n",
    "outIndice = np.sort(outIndice)\n",
    "\n",
    "scores_Letter_macro = []\n",
    "scores_Letter_micro = []\n",
    "scores_Letter_weighted = []\n",
    "for iter_ in range(10):\n",
    "    print('iter : ', iter_)\n",
    "\n",
    "    target1 = df.iloc[indice]\n",
    "    target_train1 = target1.sample(frac = 0.7)\n",
    "    target_val1 = target1.drop(target_train1.index)\n",
    "    target_train1 = target_train1.reset_index(drop=True)\n",
    "    target_val1 = target_val1.reset_index(drop=True)\n",
    "    outliers1 = df.iloc[outIndice].reset_index(drop=True)\n",
    "    testset1 = pd.concat([target_val1, outliers1]).reset_index(drop=True)\n",
    "\n",
    "    y_true1 =  [1 if  i in normal else -1 for i in testset1['class']]\n",
    "    target_train1 = target_train1.drop('class', axis=1)\n",
    "    testset1 = testset1.drop('class', axis=1)\n",
    "\n",
    "    model1 = PP4.PseudoPoints(target_train1, 10)\n",
    "    opt_comb1 = model1.search_optimal_hyperparameters()\n",
    "\n",
    "    clf1 = OneClassSVM(nu = opt_comb1[0], gamma = opt_comb1[1]).fit(target_train1)\n",
    "    y_pred1 = clf1.predict(testset1)\n",
    "    score1_macro = f1_score(y_true1, y_pred1, average='macro')\n",
    "    score1_micro = f1_score(y_true1, y_pred1, average='micro')\n",
    "    score1_weighted = f1_score(y_true1, y_pred1, average='weighted')\n",
    "\n",
    "    target2 = df.iloc[outIndice]\n",
    "    target_train2 = target2.sample(frac = 0.7)\n",
    "    target_val2 = target2.drop(target_train2.index)\n",
    "    target_train2 = target_train2.reset_index(drop=True)\n",
    "    target_val2 = target_val2.reset_index(drop=True)\n",
    "    outliers2 = df.iloc[indice].reset_index(drop=True)\n",
    "    testset2 = pd.concat([target_val2, outliers2]).reset_index(drop=True)\n",
    "\n",
    "    y_true2 =  [1 if i in out else -1 for i in testset2['class']]\n",
    "    target_train2 = target_train2.drop('class', axis=1)\n",
    "    testset2 = testset2.drop('class', axis=1)\n",
    "\n",
    "    model2 = PP4.PseudoPoints(target_train2, 10)\n",
    "    opt_comb2 = model2.search_optimal_hyperparameters()\n",
    "\n",
    "    clf2 = OneClassSVM(nu = opt_comb2[0], gamma = opt_comb2[1]).fit(target_train2)\n",
    "    y_pred2 = clf2.predict(testset2)\n",
    "    score2_macro = f1_score(y_true2, y_pred2, average='macro')\n",
    "    score2_micro = f1_score(y_true2, y_pred2, average='micro')\n",
    "    score2_weighted = f1_score(y_true2, y_pred2, average='weighted')\n",
    "\n",
    "    mean_score_macro = np.mean([score1_macro, score2_macro])\n",
    "    mean_score_micro = np.mean([score1_micro, score2_micro])\n",
    "    mean_score_weighted = np.mean([score1_weighted, score2_weighted])\n",
    "    scores_Letter_macro.append(mean_score_macro)\n",
    "    scores_Letter_micro.append(mean_score_micro)\n",
    "    scores_Letter_weighted.append(mean_score_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Abalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter :  0 , target class =  M\n",
      "iter :  1 , target class =  M\n",
      "iter :  2 , target class =  M\n",
      "iter :  3 , target class =  M\n",
      "iter :  4 , target class =  M\n",
      "iter :  5 , target class =  M\n",
      "iter :  6 , target class =  M\n",
      "iter :  7 , target class =  M\n",
      "iter :  8 , target class =  M\n",
      "iter :  9 , target class =  M\n",
      "iter :  0 , target class =  F\n",
      "iter :  1 , target class =  F\n",
      "iter :  2 , target class =  F\n",
      "iter :  3 , target class =  F\n",
      "iter :  4 , target class =  F\n",
      "iter :  5 , target class =  F\n",
      "iter :  6 , target class =  F\n",
      "iter :  7 , target class =  F\n",
      "iter :  8 , target class =  F\n",
      "iter :  9 , target class =  F\n",
      "iter :  0 , target class =  I\n",
      "iter :  1 , target class =  I\n",
      "iter :  2 , target class =  I\n",
      "iter :  3 , target class =  I\n",
      "iter :  4 , target class =  I\n",
      "iter :  5 , target class =  I\n",
      "iter :  6 , target class =  I\n",
      "iter :  7 , target class =  I\n",
      "iter :  8 , target class =  I\n",
      "iter :  9 , target class =  I\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('abalone.data', sep=',', header=None)\n",
    "X_data = df.drop(0, axis=1)\n",
    "scaler = MinMaxScaler(feature_range =(-1,1), copy=False)\n",
    "scaler.fit(X_data)\n",
    "scaled = pd.DataFrame(scaler.transform(X_data))\n",
    "scaled['class'] = df[0]\n",
    "df = scaled\n",
    "classes = df['class'].unique()\n",
    "scores_Abalone_macro = []\n",
    "scores_Abalone_micro = []\n",
    "scores_Abalone_weighted = []\n",
    "\n",
    "for c in classes:\n",
    "    for iter_ in range(10):\n",
    "        print('iter : ',iter_,', target class = ', c)\n",
    "        target1 = df[df['class']==c]\n",
    "        target_train1 = target1.sample(frac = 0.7)\n",
    "        target_val1 = target1.drop(target_train1.index)\n",
    "        target_train1 = target_train1.reset_index(drop=True)\n",
    "        target_val1 = target_val1.reset_index(drop=True)\n",
    "        outliers1 = df[df['class']!=c].reset_index(drop=True)\n",
    "        testset1 = pd.concat([target_val1, outliers1]).reset_index(drop=True)\n",
    "\n",
    "        y_true1 =  [1 if  i ==c else -1 for i in testset1['class']]\n",
    "        target_train1 = target_train1.drop('class', axis=1)\n",
    "        testset1 = testset1.drop('class', axis=1)\n",
    "\n",
    "        model1 = PP4.PseudoPoints(target_train1, 10)\n",
    "        opt_comb1 = model1.search_optimal_hyperparameters()\n",
    "\n",
    "        clf1 = OneClassSVM(nu = opt_comb1[0], gamma = opt_comb1[1]).fit(target_train1)\n",
    "        y_pred1 = clf1.predict(testset1)\n",
    "        score1_macro = f1_score(y_true1, y_pred1, average='macro')\n",
    "        score1_micro = f1_score(y_true1, y_pred1, average='micro')\n",
    "        score1_weighted = f1_score(y_true1, y_pred1, average='weighted')\n",
    "        \n",
    "        target2 = df[df['class']!=c]\n",
    "        target_train2 = target2.sample(frac = 0.7)\n",
    "        target_val2 = target2.drop(target_train2.index)\n",
    "        target_train2 = target_train2.reset_index(drop=True)\n",
    "        target_val2 = target_val2.reset_index(drop=True)\n",
    "        outliers2 = df[df['class']==c].reset_index(drop=True)\n",
    "        testset2 = pd.concat([target_val2, outliers2]).reset_index(drop=True)\n",
    "\n",
    "        y_true2 =  [1 if i!=c else -1 for i in testset2['class']]\n",
    "        target_train2 = target_train2.drop('class', axis=1)\n",
    "        testset2 = testset2.drop('class', axis=1)\n",
    "\n",
    "        model2 = PP4.PseudoPoints(target_train2, 10)\n",
    "        opt_comb2 = model2.search_optimal_hyperparameters()\n",
    "\n",
    "        clf2 = OneClassSVM(nu = opt_comb2[0], gamma = opt_comb2[1]).fit(target_train2)\n",
    "        y_pred2 = clf2.predict(testset2)\n",
    "        score2_macro = f1_score(y_true2, y_pred2, average='macro')\n",
    "        score2_micro = f1_score(y_true2, y_pred2, average='micro')\n",
    "        score2_weighted = f1_score(y_true2, y_pred2, average='weighted')\n",
    "\n",
    "        mean_score_macro = np.mean([score1_macro, score2_macro])\n",
    "        mean_score_micro = np.mean([score1_micro, score2_micro])\n",
    "        mean_score_weighted = np.mean([score1_weighted, score2_weighted])\n",
    "        \n",
    "        scores_Abalone_macro.append(mean_score_macro)\n",
    "        scores_Abalone_micro.append(mean_score_micro)\n",
    "        scores_Abalone_weighted.append(mean_score_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter :  0 , target class =  B\n",
      "iter :  1 , target class =  B\n",
      "iter :  2 , target class =  B\n",
      "iter :  3 , target class =  B\n",
      "iter :  4 , target class =  B\n",
      "iter :  5 , target class =  B\n",
      "iter :  6 , target class =  B\n",
      "iter :  7 , target class =  B\n",
      "iter :  8 , target class =  B\n",
      "iter :  9 , target class =  B\n",
      "iter :  0 , target class =  R\n",
      "iter :  1 , target class =  R\n",
      "iter :  2 , target class =  R\n",
      "iter :  3 , target class =  R\n",
      "iter :  4 , target class =  R\n",
      "iter :  5 , target class =  R\n",
      "iter :  6 , target class =  R\n",
      "iter :  7 , target class =  R\n",
      "iter :  8 , target class =  R\n",
      "iter :  9 , target class =  R\n",
      "iter :  0 , target class =  L\n",
      "iter :  1 , target class =  L\n",
      "iter :  2 , target class =  L\n",
      "iter :  3 , target class =  L\n",
      "iter :  4 , target class =  L\n",
      "iter :  5 , target class =  L\n",
      "iter :  6 , target class =  L\n",
      "iter :  7 , target class =  L\n",
      "iter :  8 , target class =  L\n",
      "iter :  9 , target class =  L\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('balance-scale.data', sep=',', header=None)\n",
    "X_data = df.drop(0, axis=1)\n",
    "scaler = MinMaxScaler(feature_range =(-1,1))\n",
    "scaler.fit(X_data)\n",
    "scaled = pd.DataFrame(scaler.transform(X_data))\n",
    "scaled['class'] = df[0]\n",
    "df = scaled\n",
    "classes = df['class'].unique()\n",
    "\n",
    "scores_Balance_macro = []\n",
    "scores_Balance_micro = []\n",
    "scores_Balance_weighted = []\n",
    "\n",
    "for c in classes:\n",
    "    for iter_ in range(10):\n",
    "        print('iter : ', iter_,', target class = ', c)\n",
    "        target1 = df[df['class']==c]\n",
    "        target_train1 = target1.sample(frac = 0.7)\n",
    "        target_val1 = target1.drop(target_train1.index)\n",
    "        target_train1 = target_train1.reset_index(drop=True)\n",
    "        target_val1 = target_val1.reset_index(drop=True)\n",
    "        outliers1 = df[df['class']!=c].reset_index(drop=True)\n",
    "        testset1 = pd.concat([target_val1, outliers1]).reset_index(drop=True)\n",
    "\n",
    "        y_true1 =  [1 if  i ==c else -1 for i in testset1['class']]\n",
    "        target_train1 = target_train1.drop('class', axis=1)\n",
    "        testset1 = testset1.drop('class', axis=1)\n",
    "\n",
    "        model1 = PP4.PseudoPoints(target_train1, 10)\n",
    "        opt_comb1 = model1.search_optimal_hyperparameters()\n",
    "\n",
    "        clf1 = OneClassSVM(nu = opt_comb1[0], gamma = opt_comb1[1]).fit(target_train1)\n",
    "        y_pred1 = clf1.predict(testset1)\n",
    "        \n",
    "        score1_macro = f1_score(y_true1, y_pred1, average='macro')\n",
    "        score1_micro = f1_score(y_true1, y_pred1, average='micro')\n",
    "        score1_weighted = f1_score(y_true1, y_pred1, average='weighted')\n",
    "        \n",
    "        target2 = df[df['class']!=c]\n",
    "        target_train2 = target2.sample(frac = 0.7)\n",
    "        target_val2 = target2.drop(target_train2.index)\n",
    "        target_train2 = target_train2.reset_index(drop=True)\n",
    "        target_val2 = target_val2.reset_index(drop=True)\n",
    "        outliers2 = df[df['class']==c].reset_index(drop=True)\n",
    "        testset2 = pd.concat([target_val2, outliers2]).reset_index(drop=True)\n",
    "\n",
    "        y_true2 =  [1 if i!=c else -1 for i in testset2['class']]\n",
    "        target_train2 = target_train2.drop('class', axis=1)\n",
    "        testset2 = testset2.drop('class', axis=1)\n",
    "\n",
    "        model2 = PP4.PseudoPoints(target_train2, 10)\n",
    "        opt_comb2 = model2.search_optimal_hyperparameters()\n",
    "\n",
    "        clf2 = OneClassSVM(nu = opt_comb2[0], gamma = opt_comb2[1]).fit(target_train2)\n",
    "        y_pred2 = clf2.predict(testset2)\n",
    "        score2_macro = f1_score(y_true2, y_pred2, average='macro')\n",
    "        score2_micro = f1_score(y_true2, y_pred2, average='micro')\n",
    "        score2_weighted = f1_score(y_true2, y_pred2, average='weighted')\n",
    "\n",
    "        mean_score_macro = np.mean([score1_macro, score2_macro])\n",
    "        mean_score_micro = np.mean([score1_micro, score2_micro])\n",
    "        mean_score_weighted = np.mean([score1_weighted, score2_weighted])\n",
    "        \n",
    "        scores_Balance_macro.append(mean_score_macro)\n",
    "        scores_Balance_micro.append(mean_score_micro)\n",
    "        scores_Balance_weighted.append(mean_score_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Heart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter :  0 , target class =  2\n",
      "iter :  1 , target class =  2\n",
      "iter :  2 , target class =  2\n",
      "iter :  3 , target class =  2\n",
      "iter :  4 , target class =  2\n",
      "iter :  5 , target class =  2\n",
      "iter :  6 , target class =  2\n",
      "iter :  7 , target class =  2\n",
      "iter :  8 , target class =  2\n",
      "iter :  9 , target class =  2\n",
      "iter :  0 , target class =  1\n",
      "iter :  1 , target class =  1\n",
      "iter :  2 , target class =  1\n",
      "iter :  3 , target class =  1\n",
      "iter :  4 , target class =  1\n",
      "iter :  5 , target class =  1\n",
      "iter :  6 , target class =  1\n",
      "iter :  7 , target class =  1\n",
      "iter :  8 , target class =  1\n",
      "iter :  9 , target class =  1\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('heart.dat', sep = ' ',  header=None)\n",
    "X_data = df.drop(13, axis=1)\n",
    "scaler = MinMaxScaler(feature_range =(-1,1))\n",
    "scaler.fit(X_data)\n",
    "scaled = pd.DataFrame(scaler.transform(X_data))\n",
    "scaled['class'] = df[13]\n",
    "df = scaled\n",
    "classes = df['class'].unique()\n",
    "scores_Heart_macro = []\n",
    "scores_Heart_micro = []\n",
    "scores_Heart_weighted = []\n",
    "for c in classes:\n",
    "    for iter_ in range(10):\n",
    "        print('iter : ', iter_,', target class = ', c)\n",
    "        target1 = df[df['class']==c]\n",
    "        target_train1 = target1.sample(frac = 0.7)\n",
    "        target_val1 = target1.drop(target_train1.index)\n",
    "        target_train1 = target_train1.reset_index(drop=True)\n",
    "        target_val1 = target_val1.reset_index(drop=True)\n",
    "        outliers1 = df[df['class']!=c].reset_index(drop=True)\n",
    "        testset1 = pd.concat([target_val1, outliers1]).reset_index(drop=True)\n",
    "\n",
    "        y_true1 =  [1 if  i ==c else -1 for i in testset1['class']]\n",
    "        target_train1 = target_train1.drop('class', axis=1)\n",
    "        testset1 = testset1.drop('class', axis=1)\n",
    "        model1 = PP4.PseudoPoints(target_train1, 10)\n",
    "        opt_comb1 = model1.search_optimal_hyperparameters()\n",
    "\n",
    "        clf1 = OneClassSVM(nu = opt_comb1[0], gamma = opt_comb1[1]).fit(target_train1)\n",
    "        y_pred1 = clf1.predict(testset1)\n",
    "        score1_macro = f1_score(y_true1, y_pred1, average='macro')\n",
    "        score1_micro = f1_score(y_true1, y_pred1, average='micro')\n",
    "        score1_weighted = f1_score(y_true1, y_pred1, average='weighted')\n",
    "        \n",
    "        target2 = df[df['class']!=c]\n",
    "        target_train2 = target2.sample(frac = 0.7)\n",
    "        target_val2 = target2.drop(target_train2.index)\n",
    "        target_train2 = target_train2.reset_index(drop=True)\n",
    "        target_val2 = target_val2.reset_index(drop=True)\n",
    "        outliers2 = df[df['class']==c].reset_index(drop=True)\n",
    "        testset2 = pd.concat([target_val2, outliers2]).reset_index(drop=True)\n",
    "\n",
    "        y_true2 =  [1 if i!=c else -1 for i in testset2['class']]\n",
    "        target_train2 = target_train2.drop('class', axis=1)\n",
    "        testset2 = testset2.drop('class', axis=1)\n",
    "\n",
    "        model2 = PP4.PseudoPoints(target_train2, 10)\n",
    "        opt_comb2 = model2.search_optimal_hyperparameters()\n",
    "\n",
    "        clf2 = OneClassSVM(nu = opt_comb2[0], gamma = opt_comb2[1]).fit(target_train2)\n",
    "        y_pred2 = clf2.predict(testset2)\n",
    "        score2_macro = f1_score(y_true2, y_pred2, average='macro')\n",
    "        score2_micro = f1_score(y_true2, y_pred2, average='micro')\n",
    "        score2_weighted = f1_score(y_true2, y_pred2, average='weighted')\n",
    "\n",
    "        mean_score_macro = np.mean([score1_macro, score2_macro])\n",
    "        mean_score_micro = np.mean([score1_micro, score2_micro])\n",
    "        mean_score_weighted = np.mean([score1_weighted, score2_weighted])\n",
    "\n",
    "        scores_Heart_macro.append(mean_score_macro)\n",
    "        scores_Heart_micro.append(mean_score_micro)\n",
    "        scores_Heart_weighted.append(mean_score_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Australian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter :  0 , target class =  0\n",
      "iter :  1 , target class =  0\n",
      "iter :  2 , target class =  0\n",
      "iter :  3 , target class =  0\n",
      "iter :  4 , target class =  0\n",
      "iter :  5 , target class =  0\n",
      "iter :  6 , target class =  0\n",
      "iter :  7 , target class =  0\n",
      "iter :  8 , target class =  0\n",
      "iter :  9 , target class =  0\n",
      "iter :  0 , target class =  1\n",
      "iter :  1 , target class =  1\n",
      "iter :  2 , target class =  1\n",
      "iter :  3 , target class =  1\n",
      "iter :  4 , target class =  1\n",
      "iter :  5 , target class =  1\n",
      "iter :  6 , target class =  1\n",
      "iter :  7 , target class =  1\n",
      "iter :  8 , target class =  1\n",
      "iter :  9 , target class =  1\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('australian.dat', sep = ' ',  header=None)\n",
    "X_data = df.drop(14, axis=1)\n",
    "scaler = MinMaxScaler(feature_range =(-1,1))\n",
    "scaler.fit(X_data)\n",
    "scaled = pd.DataFrame(scaler.transform(X_data))\n",
    "scaled['class'] = df[14]\n",
    "df = scaled\n",
    "classes = df['class'].unique()\n",
    "scores_Australian_macro = []\n",
    "scores_Australian_micro = []\n",
    "scores_Australian_weighted = []\n",
    "for c in classes:\n",
    "    for iter_ in range(10):\n",
    "        print('iter : ', iter_,', target class = ', c)\n",
    "        \n",
    "        target1 = df[df['class']==c]\n",
    "        target_train1 = target1.sample(frac = 0.7)\n",
    "        target_val1 = target1.drop(target_train1.index)\n",
    "        target_train1 = target_train1.reset_index(drop=True)\n",
    "        target_val1 = target_val1.reset_index(drop=True)\n",
    "        outliers1 = df[df['class']!=c].reset_index(drop=True)\n",
    "        testset1 = pd.concat([target_val1, outliers1]).reset_index(drop=True)\n",
    "\n",
    "        y_true1 =  [1 if  i ==c else -1 for i in testset1['class']]\n",
    "        target_train1 = target_train1.drop('class', axis=1)\n",
    "        testset1 = testset1.drop('class', axis=1)\n",
    "        \n",
    "        model1 = PP4.PseudoPoints(target_train1, 10)\n",
    "        opt_comb1 = model1.search_optimal_hyperparameters()\n",
    "\n",
    "        clf1 = OneClassSVM(nu = opt_comb1[0], gamma = opt_comb1[1]).fit(target_train1)\n",
    "        y_pred1 = clf1.predict(testset1)\n",
    "        score1_macro = f1_score(y_true1, y_pred1, average='macro')\n",
    "        score1_micro = f1_score(y_true1, y_pred1, average='micro')\n",
    "        score1_weighted = f1_score(y_true1, y_pred1, average='weighted')\n",
    "\n",
    "\n",
    "        target2 = df[df['class']!=c]\n",
    "        target_train2 = target2.sample(frac = 0.7)\n",
    "        target_val2 = target2.drop(target_train2.index)\n",
    "        target_train2 = target_train2.reset_index(drop=True)\n",
    "        target_val2 = target_val2.reset_index(drop=True)\n",
    "        outliers2 = df[df['class']==c].reset_index(drop=True)\n",
    "        testset2 = pd.concat([target_val2, outliers2]).reset_index(drop=True)\n",
    "\n",
    "        y_true2 =  [1 if i!=c else -1 for i in testset2['class']]\n",
    "        target_train2 = target_train2.drop('class', axis=1)\n",
    "        testset2 = testset2.drop('class', axis=1)\n",
    "\n",
    "        model2 = PP4.PseudoPoints(target_train2, 10)\n",
    "        opt_comb2 = model2.search_optimal_hyperparameters()\n",
    "\n",
    "        clf2 = OneClassSVM(nu = opt_comb2[0], gamma = opt_comb2[1]).fit(target_train2)\n",
    "        y_pred2 = clf2.predict(testset2)\n",
    "        score2_macro = f1_score(y_true2, y_pred2, average='macro')\n",
    "        score2_micro = f1_score(y_true2, y_pred2, average='micro')\n",
    "        score2_weighted = f1_score(y_true2, y_pred2, average='weighted')\n",
    "\n",
    "\n",
    "        mean_score_macro = np.mean([score1_macro, score2_macro])\n",
    "        mean_score_micro = np.mean([score1_micro, score2_micro])\n",
    "        mean_score_weighted = np.mean([score1_weighted, score2_weighted])\n",
    "        scores_Australian_macro.append(mean_score_macro)\n",
    "        scores_Australian_micro.append(mean_score_micro)\n",
    "        scores_Australian_weighted.append(mean_score_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Glass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter :  0\n",
      "iter :  1\n",
      "iter :  2\n",
      "iter :  3\n",
      "iter :  4\n",
      "iter :  5\n",
      "iter :  6\n",
      "iter :  7\n",
      "iter :  8\n",
      "iter :  9\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('glass.data', sep = ',',  header=None)\n",
    "df = df.drop(0, axis=1)\n",
    "X_data = df.drop(10, axis=1)\n",
    "scaler = MinMaxScaler(feature_range =(-1,1))\n",
    "scaler.fit(X_data)\n",
    "scaled = pd.DataFrame(scaler.transform(X_data))\n",
    "scaled['class'] = df[10]\n",
    "df = scaled\n",
    "classes = df['class'].unique()\n",
    "normal = [1,2,3]\n",
    "out = [5,6,7]\n",
    "indexes = []\n",
    "outIndexes = []\n",
    "for c in normal:\n",
    "    indexes += df['class'].index[df['class'] == c].tolist()\n",
    "\n",
    "for c in out:\n",
    "    outIndexes += df['class'].index[df['class'] == c].tolist()\n",
    "\n",
    "indexes = np.sort(indexes)\n",
    "outIndexes = np.sort(outIndexes)\n",
    "classes = [normal, out]\n",
    "\n",
    "scores_Glass_macro = []\n",
    "scores_Glass_micro = []\n",
    "scores_Glass_weighted = []\n",
    "\n",
    "for iter_ in range(10):\n",
    "    print('iter : ', iter_)\n",
    "\n",
    "    target1 = df.iloc[indexes]\n",
    "    target_train1 = target1.sample(frac = 0.7)\n",
    "    target_val1 = target1.drop(target_train1.index)\n",
    "    target_train1 = target_train1.reset_index(drop=True)\n",
    "    target_val1 = target_val1.reset_index(drop=True)\n",
    "    outliers1 = df.iloc[outIndexes].reset_index(drop=True)\n",
    "    testset1 = pd.concat([target_val1, outliers1]).reset_index(drop=True)\n",
    "\n",
    "    y_true1 =  [1 if  i in normal else -1 for i in testset1['class']]\n",
    "    target_train1 = target_train1.drop('class', axis=1)\n",
    "    testset1 = testset1.drop('class', axis=1)\n",
    "\n",
    "    model1 = PP4.PseudoPoints(target_train1, 10)\n",
    "    opt_comb1 = model1.search_optimal_hyperparameters()\n",
    "\n",
    "    clf1 = OneClassSVM(nu = opt_comb1[0], gamma = opt_comb1[1]).fit(target_train1)\n",
    "    y_pred1 = clf1.predict(testset1)\n",
    "    score1_macro = f1_score(y_true1, y_pred1, average='macro')\n",
    "    score1_micro = f1_score(y_true1, y_pred1, average='micro')\n",
    "    score1_weighted = f1_score(y_true1, y_pred1, average='weighted')\n",
    "\n",
    "    target2 = df.iloc[outIndexes]\n",
    "    target_train2 = target2.sample(frac = 0.7)\n",
    "    target_val2 = target2.drop(target_train2.index)\n",
    "    target_train2 = target_train2.reset_index(drop=True)\n",
    "    target_val2 = target_val2.reset_index(drop=True)\n",
    "    outliers2 = df.iloc[indexes].reset_index(drop=True)\n",
    "    testset2 = pd.concat([target_val2, outliers2]).reset_index(drop=True)\n",
    "\n",
    "    y_true2 =  [1 if i in out else -1 for i in testset2['class']]\n",
    "    target_train2 = target_train2.drop('class', axis=1)\n",
    "    testset2 = testset2.drop('class', axis=1)\n",
    "\n",
    "    model2 = PP4.PseudoPoints(target_train2, 10)\n",
    "    opt_comb2 = model2.search_optimal_hyperparameters()\n",
    "\n",
    "    clf2 = OneClassSVM(nu = opt_comb2[0], gamma = opt_comb2[1]).fit(target_train2)\n",
    "    y_pred2 = clf2.predict(testset2)\n",
    "    score2_macro = f1_score(y_true2, y_pred2, average='macro')\n",
    "    score2_micro = f1_score(y_true2, y_pred2, average='micro')\n",
    "    score2_weighted = f1_score(y_true2, y_pred2, average='weighted')\n",
    "\n",
    "    mean_score_macro = np.mean([score1_macro, score2_macro])\n",
    "    mean_score_micro = np.mean([score1_micro, score2_micro])\n",
    "    mean_score_weighted = np.mean([score1_weighted, score2_weighted])\n",
    "    \n",
    "    scores_Glass_macro.append(mean_score_macro)\n",
    "    scores_Glass_micro.append(mean_score_micro)\n",
    "    scores_Glass_weighted.append(mean_score_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Landsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter :  0\n",
      "iter :  1\n",
      "iter :  2\n",
      "iter :  3\n",
      "iter :  4\n",
      "iter :  5\n",
      "iter :  6\n",
      "iter :  7\n",
      "iter :  8\n",
      "iter :  9\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('bmdatasets\\sat.tst', sep = ' ', header=None)\n",
    "X_data = df.drop(36, axis=1)\n",
    "scaler = MinMaxScaler(feature_range =(-1,1))\n",
    "scaler.fit(X_data)\n",
    "scaled = pd.DataFrame(scaler.transform(X_data))\n",
    "scaled['class'] = df[36]\n",
    "df = scaled\n",
    "classes = df['class'].unique()\n",
    "normal = [1,2,3]\n",
    "out = [4,5,7]\n",
    "indice = []\n",
    "outIndice = []\n",
    "for c in normal:\n",
    "    indice += df['class'].index[df['class'] == c].tolist()\n",
    "\n",
    "for c in out:\n",
    "    outIndice += df['class'].index[df['class'] == c].tolist()\n",
    "\n",
    "indice = np.sort(indice)\n",
    "outIndice = np.sort(outIndice)\n",
    "\n",
    "scores_Landsat_macro = []\n",
    "scores_Landsat_micro = []\n",
    "scores_Landsat_weighted = []\n",
    "\n",
    "for iter_ in range(10):\n",
    "    print('iter : ', iter_)\n",
    "\n",
    "    target1 = df.iloc[indice]\n",
    "    target_train1 = target1.sample(frac = 0.7)\n",
    "    target_val1 = target1.drop(target_train1.index)\n",
    "    target_train1 = target_train1.reset_index(drop=True)\n",
    "    target_val1 = target_val1.reset_index(drop=True)\n",
    "    outliers1 = df.iloc[outIndice].reset_index(drop=True)\n",
    "    testset1 = pd.concat([target_val1, outliers1]).reset_index(drop=True)\n",
    "\n",
    "    y_true1 =  [1 if  i in normal else -1 for i in testset1['class']]\n",
    "    target_train1 = target_train1.drop('class', axis=1)\n",
    "    testset1 = testset1.drop('class', axis=1)\n",
    "\n",
    "    model1 = PP4.PseudoPoints(target_train1, 10)\n",
    "    opt_comb1 = model1.search_optimal_hyperparameters()\n",
    "\n",
    "    clf1 = OneClassSVM(nu = opt_comb1[0], gamma = opt_comb1[1]).fit(target_train1)\n",
    "    y_pred1 = clf1.predict(testset1)\n",
    "    score1_macro = f1_score(y_true1, y_pred1, average='macro')\n",
    "    score1_micro = f1_score(y_true1, y_pred1, average='micro')\n",
    "    score1_weighted = f1_score(y_true1, y_pred1, average='weighted')\n",
    "\n",
    "    target2 = df.iloc[outIndice]\n",
    "    target_train2 = target2.sample(frac = 0.7)\n",
    "    target_val2 = target2.drop(target_train2.index)\n",
    "    target_train2 = target_train2.reset_index(drop=True)\n",
    "    target_val2 = target_val2.reset_index(drop=True)\n",
    "    outliers2 = df.iloc[indice].reset_index(drop=True)\n",
    "    testset2 = pd.concat([target_val2, outliers2]).reset_index(drop=True)\n",
    "\n",
    "    y_true2 =  [1 if i in out else -1 for i in testset2['class']]\n",
    "    target_train2 = target_train2.drop('class', axis=1)\n",
    "    testset2 = testset2.drop('class', axis=1)\n",
    "\n",
    "    model2 = PP4.PseudoPoints(target_train2, 10)\n",
    "    opt_comb2 = model2.search_optimal_hyperparameters()\n",
    "\n",
    "    clf2 = OneClassSVM(nu = opt_comb2[0], gamma = opt_comb2[1]).fit(target_train2)\n",
    "    y_pred2 = clf2.predict(testset2)\n",
    "    score2_macro = f1_score(y_true2, y_pred2, average='macro')\n",
    "    score2_micro = f1_score(y_true2, y_pred2, average='micro')\n",
    "    score2_weighted = f1_score(y_true2, y_pred2, average='weighted')\n",
    "\n",
    "\n",
    "    \n",
    "    mean_score_macro = np.mean([score1_macro, score2_macro])\n",
    "    mean_score_micro = np.mean([score1_micro, score2_micro])\n",
    "    mean_score_weighted = np.mean([score1_weighted, score2_weighted])\n",
    "\n",
    "    scores_Landsat_macro.append(mean_score_macro)\n",
    "    scores_Landsat_micro.append(mean_score_micro)\n",
    "    scores_Landsat_weighted.append(mean_score_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter :  0\n",
      "iter :  1\n",
      "iter :  2\n",
      "iter :  3\n",
      "iter :  4\n",
      "iter :  5\n",
      "iter :  6\n",
      "iter :  7\n",
      "iter :  8\n",
      "iter :  9\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('bmdatasets/segment.dat', sep=' ', header=None)\n",
    "X_data = df.drop(19, axis=1)\n",
    "scaler = MinMaxScaler(feature_range =(-1,1))\n",
    "scaler.fit(X_data)\n",
    "scaled = pd.DataFrame(scaler.transform(X_data))\n",
    "scaled['class'] = df[19]\n",
    "df = scaled\n",
    "classes = df['class'].unique()\n",
    "normal = [1,2,3,4]\n",
    "out = [5,6,7]\n",
    "indice = []\n",
    "outIndice = []\n",
    "for c in normal:\n",
    "    indice += df['class'].index[df['class'] == c].tolist()\n",
    "\n",
    "for c in out:\n",
    "    outIndice += df['class'].index[df['class'] == c].tolist()\n",
    "\n",
    "indice = np.sort(indice)\n",
    "outIndice = np.sort(outIndice)\n",
    "classes = [normal, out]\n",
    "scores_Segment_macro = []\n",
    "scores_Segment_micro = []\n",
    "scores_Segment_weighted = []\n",
    "\n",
    "for iter_ in range(10):\n",
    "    print('iter : ', iter_)\n",
    "\n",
    "    target1 = df.iloc[indice]\n",
    "    target_train1 = target1.sample(frac = 0.7)\n",
    "    target_val1 = target1.drop(target_train1.index)\n",
    "    target_train1 = target_train1.reset_index(drop=True)\n",
    "    target_val1 = target_val1.reset_index(drop=True)\n",
    "    outliers1 = df.iloc[outIndice].reset_index(drop=True)\n",
    "    testset1 = pd.concat([target_val1, outliers1]).reset_index(drop=True)\n",
    "\n",
    "    y_true1 =  [1 if  i in normal else -1 for i in testset1['class']]\n",
    "    target_train1 = target_train1.drop('class', axis=1)\n",
    "    testset1 = testset1.drop('class', axis=1)\n",
    "\n",
    "    model1 = PP4.PseudoPoints(target_train1, 10)\n",
    "    opt_comb1 = model1.search_optimal_hyperparameters()\n",
    "\n",
    "    clf1 = OneClassSVM(nu = opt_comb1[0], gamma = opt_comb1[1]).fit(target_train1)\n",
    "    y_pred1 = clf1.predict(testset1)\n",
    "    \n",
    "\n",
    "    target2 = df.iloc[outIndice]\n",
    "    target_train2 = target2.sample(frac = 0.7)\n",
    "    target_val2 = target2.drop(target_train2.index)\n",
    "    target_train2 = target_train2.reset_index(drop=True)\n",
    "    target_val2 = target_val2.reset_index(drop=True)\n",
    "    outliers2 = df.iloc[indice].reset_index(drop=True)\n",
    "    testset2 = pd.concat([target_val2, outliers2]).reset_index(drop=True)\n",
    "\n",
    "    y_true2 =  [1 if i in out else -1 for i in testset2['class']]\n",
    "    target_train2 = target_train2.drop('class', axis=1)\n",
    "    testset2 = testset2.drop('class', axis=1)\n",
    "\n",
    "    model2 = PP4.PseudoPoints(target_train2, 10)\n",
    "    opt_comb2 = model2.search_optimal_hyperparameters()\n",
    "\n",
    "    clf2 = OneClassSVM(nu = opt_comb2[0], gamma = opt_comb2[1]).fit(target_train2)\n",
    "    y_pred2 = clf2.predict(testset2)\n",
    "    \n",
    "\n",
    "    score1_macro = f1_score(y_true1, y_pred1, average='macro')\n",
    "    score1_micro = f1_score(y_true1, y_pred1, average='micro')\n",
    "    score1_weighted = f1_score(y_true1, y_pred1, average='weighted')\n",
    "\n",
    "    score2_macro = f1_score(y_true2, y_pred2, average='macro')\n",
    "    score2_micro = f1_score(y_true2, y_pred2, average='micro')\n",
    "    score2_weighted = f1_score(y_true2, y_pred2, average='weighted')\n",
    "\n",
    "    mean_score_macro = np.mean([score1_macro, score2_macro])\n",
    "    mean_score_micro = np.mean([score1_micro, score2_micro])\n",
    "    mean_score_weighted = np.mean([score1_weighted, score2_weighted])\n",
    "\n",
    "    scores_Segment_macro.append(mean_score_macro)\n",
    "    scores_Segment_micro.append(mean_score_micro)\n",
    "    scores_Segment_weighted.append(mean_score_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sonar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter :  0 , target class =  R\n",
      "iter :  1 , target class =  R\n",
      "iter :  2 , target class =  R\n",
      "iter :  3 , target class =  R\n",
      "iter :  4 , target class =  R\n",
      "iter :  5 , target class =  R\n",
      "iter :  6 , target class =  R\n",
      "iter :  7 , target class =  R\n",
      "iter :  8 , target class =  R\n",
      "iter :  9 , target class =  R\n",
      "iter :  0 , target class =  M\n",
      "iter :  1 , target class =  M\n",
      "iter :  2 , target class =  M\n",
      "iter :  3 , target class =  M\n",
      "iter :  4 , target class =  M\n",
      "iter :  5 , target class =  M\n",
      "iter :  6 , target class =  M\n",
      "iter :  7 , target class =  M\n",
      "iter :  8 , target class =  M\n",
      "iter :  9 , target class =  M\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('bmdatasets/sonar.all-data', header=None)\n",
    "X_data = df.drop(60, axis=1)\n",
    "scaler = MinMaxScaler(feature_range =(-1,1))\n",
    "scaler.fit(X_data)\n",
    "scaled = pd.DataFrame(scaler.transform(X_data))\n",
    "scaled['class'] = df[60]\n",
    "df = scaled\n",
    "classes = df['class'].unique()\n",
    "scores_Sonar_macro = []\n",
    "scores_Sonar_micro = []\n",
    "scores_Sonar_weighted = []\n",
    "for c in classes:\n",
    "    for iter_ in range(10):\n",
    "        print('iter : ', iter_,', target class = ', c)\n",
    "        \n",
    "        target1 = df[df['class']==c]\n",
    "        target_train1 = target1.sample(frac = 0.7)\n",
    "        target_val1 = target1.drop(target_train1.index)\n",
    "        target_train1 = target_train1.reset_index(drop=True)\n",
    "        target_val1 = target_val1.reset_index(drop=True)\n",
    "        outliers1 = df[df['class']!=c].reset_index(drop=True)\n",
    "        testset1 = pd.concat([target_val1, outliers1]).reset_index(drop=True)\n",
    "\n",
    "        y_true1 =  [1 if  i ==c else -1 for i in testset1['class']]\n",
    "        target_train1 = target_train1.drop('class', axis=1)\n",
    "        testset1 = testset1.drop('class', axis=1)\n",
    "        \n",
    "        model1 = PP4.PseudoPoints(target_train1, 10)\n",
    "        opt_comb1 = model1.search_optimal_hyperparameters()\n",
    "\n",
    "        clf1 = OneClassSVM(nu = opt_comb1[0], gamma = opt_comb1[1]).fit(target_train1)\n",
    "        y_pred1 = clf1.predict(testset1)\n",
    "        \n",
    "\n",
    "        target2 = df[df['class']!=c]\n",
    "        target_train2 = target2.sample(frac = 0.7)\n",
    "        target_val2 = target2.drop(target_train2.index)\n",
    "        target_train2 = target_train2.reset_index(drop=True)\n",
    "        target_val2 = target_val2.reset_index(drop=True)\n",
    "        outliers2 = df[df['class']==c].reset_index(drop=True)\n",
    "        testset2 = pd.concat([target_val2, outliers2]).reset_index(drop=True)\n",
    "\n",
    "        y_true2 =  [1 if i!=c else -1 for i in testset2['class']]\n",
    "        target_train2 = target_train2.drop('class', axis=1)\n",
    "        testset2 = testset2.drop('class', axis=1)\n",
    "\n",
    "        model2 = PP4.PseudoPoints(target_train2, 10)\n",
    "        opt_comb2 = model2.search_optimal_hyperparameters()\n",
    "\n",
    "        clf2 = OneClassSVM(nu = opt_comb2[0], gamma = opt_comb2[1]).fit(target_train2)\n",
    "        y_pred2 = clf2.predict(testset2)\n",
    "        \n",
    "\n",
    "        score1_macro = f1_score(y_true1, y_pred1, average='macro')\n",
    "        score1_micro = f1_score(y_true1, y_pred1, average='micro')\n",
    "        score1_weighted = f1_score(y_true1, y_pred1, average='weighted')\n",
    "\n",
    "        score2_macro = f1_score(y_true2, y_pred2, average='macro')\n",
    "        score2_micro = f1_score(y_true2, y_pred2, average='micro')\n",
    "        score2_weighted = f1_score(y_true2, y_pred2, average='weighted')\n",
    "\n",
    "        mean_score_macro = np.mean([score1_macro, score2_macro])\n",
    "        mean_score_micro = np.mean([score1_micro, score2_micro])\n",
    "        mean_score_weighted = np.mean([score1_weighted, score2_weighted])\n",
    "\n",
    "        scores_Sonar_macro.append(mean_score_macro)\n",
    "        scores_Sonar_micro.append(mean_score_micro)\n",
    "        scores_Sonar_weighted.append(mean_score_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfiles = ['xaa.dat', 'xab.dat','xac.dat','xad.dat','xae.dat','xaf.dat','xag.dat','xah.dat','xai.dat']\\ndf = pd.DataFrame()\\nfor file in files:\\n    print(file)\\n    d = pd.read_csv('bmdatasets/'+file, header=None, sep=' ', error_bad_lines=False)\\n    d = d.iloc[:, :19]\\n    df = pd.concat([df, d]).reset_index(drop=True)\\n\""
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "files = ['xaa.dat', 'xab.dat','xac.dat','xad.dat','xae.dat','xaf.dat','xag.dat','xah.dat','xai.dat']\n",
    "df = pd.DataFrame()\n",
    "for file in files:\n",
    "    print(file)\n",
    "    d = pd.read_csv('bmdatasets/'+file, header=None, sep=' ', error_bad_lines=False)\n",
    "    d = d.iloc[:, :19]\n",
    "    df = pd.concat([df, d]).reset_index(drop=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('vehicle.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter :  0\n",
      "iter :  1\n",
      "iter :  2\n",
      "iter :  3\n",
      "iter :  4\n",
      "iter :  5\n",
      "iter :  6\n",
      "iter :  7\n",
      "iter :  8\n",
      "iter :  9\n"
     ]
    }
   ],
   "source": [
    "X_data = df.drop('18', axis=1)\n",
    "scaler = MinMaxScaler(feature_range =(-1,1))\n",
    "scaler.fit(X_data)\n",
    "scaled = pd.DataFrame(scaler.transform(X_data))\n",
    "scaled['class'] = df['18']\n",
    "df = scaled\n",
    "classes = df['class'].unique()\n",
    "normal = ['van', 'saab']\n",
    "out = ['bus', 'opel']\n",
    "indice = []\n",
    "outIndice = []\n",
    "for c in normal:\n",
    "    indice += df['class'].index[df['class'] == c].tolist()\n",
    "\n",
    "for c in out:\n",
    "    outIndice += df['class'].index[df['class'] == c].tolist()\n",
    "\n",
    "indice = np.sort(indice)\n",
    "outIndice = np.sort(outIndice)\n",
    "\n",
    "scores_Vehicle_macro = []\n",
    "scores_Vehicle_micro = []\n",
    "scores_Vehicle_weighted = []\n",
    "\n",
    "for iter_ in range(10):\n",
    "    print('iter : ', iter_)\n",
    "\n",
    "    target1 = df.iloc[indice]\n",
    "    target_train1 = target1.sample(frac = 0.7)\n",
    "    target_val1 = target1.drop(target_train1.index)\n",
    "    target_train1 = target_train1.reset_index(drop=True)\n",
    "    target_val1 = target_val1.reset_index(drop=True)\n",
    "    outliers1 = df.iloc[outIndice].reset_index(drop=True)\n",
    "    testset1 = pd.concat([target_val1, outliers1]).reset_index(drop=True)\n",
    "\n",
    "    y_true1 =  [1 if  i in normal else -1 for i in testset1['class']]\n",
    "    target_train1 = target_train1.drop('class', axis=1)\n",
    "    testset1 = testset1.drop('class', axis=1)\n",
    "\n",
    "    model1 = PP4.PseudoPoints(target_train1, 10)\n",
    "    opt_comb1 = model1.search_optimal_hyperparameters()\n",
    "\n",
    "    clf1 = OneClassSVM(nu = opt_comb1[0], gamma = opt_comb1[1]).fit(target_train1)\n",
    "    y_pred1 = clf1.predict(testset1)\n",
    "    \n",
    "    target2 = df.iloc[outIndice]\n",
    "    target_train2 = target2.sample(frac = 0.7)\n",
    "    target_val2 = target2.drop(target_train2.index)\n",
    "    target_train2 = target_train2.reset_index(drop=True)\n",
    "    target_val2 = target_val2.reset_index(drop=True)\n",
    "    outliers2 = df.iloc[indice].reset_index(drop=True)\n",
    "    testset2 = pd.concat([target_val2, outliers2]).reset_index(drop=True)\n",
    "\n",
    "    y_true2 =  [1 if i in out else -1 for i in testset2['class']]\n",
    "    target_train2 = target_train2.drop('class', axis=1)\n",
    "    testset2 = testset2.drop('class', axis=1)\n",
    "\n",
    "    model2 = PP4.PseudoPoints(target_train2, 10)\n",
    "    opt_comb2 = model2.search_optimal_hyperparameters()\n",
    "\n",
    "    clf2 = OneClassSVM(nu = opt_comb2[0], gamma = opt_comb2[1]).fit(target_train2)\n",
    "    y_pred2 = clf2.predict(testset2)\n",
    "    \n",
    "\n",
    "    score1_macro = f1_score(y_true1, y_pred1, average='macro')\n",
    "    score1_micro = f1_score(y_true1, y_pred1, average='micro')\n",
    "    score1_weighted = f1_score(y_true1, y_pred1, average='weighted')\n",
    "\n",
    "    score2_macro = f1_score(y_true2, y_pred2, average='macro')\n",
    "    score2_micro = f1_score(y_true2, y_pred2, average='micro')\n",
    "    score2_weighted = f1_score(y_true2, y_pred2, average='weighted')\n",
    "\n",
    "    mean_score_macro = np.mean([score1_macro, score2_macro])\n",
    "    mean_score_micro = np.mean([score1_micro, score2_micro])\n",
    "    mean_score_weighted = np.mean([score1_weighted, score2_weighted])\n",
    "\n",
    "    scores_Vehicle_macro.append(mean_score_macro)\n",
    "    scores_Vehicle_micro.append(mean_score_micro)\n",
    "    scores_Vehicle_weighted.append(mean_score_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.Waveform3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter :  0 , target class =  2 , outlier class =  [1 0]\n",
      "iter :  0 , target class =  [1 0] , outlier class =  2\n",
      "iter :  1 , target class =  2 , outlier class =  [1 0]\n",
      "iter :  1 , target class =  [1 0] , outlier class =  2\n",
      "iter :  2 , target class =  2 , outlier class =  [1 0]\n",
      "iter :  2 , target class =  [1 0] , outlier class =  2\n",
      "iter :  3 , target class =  2 , outlier class =  [1 0]\n",
      "iter :  3 , target class =  [1 0] , outlier class =  2\n",
      "iter :  4 , target class =  2 , outlier class =  [1 0]\n",
      "iter :  4 , target class =  [1 0] , outlier class =  2\n",
      "iter :  5 , target class =  2 , outlier class =  [1 0]\n",
      "iter :  5 , target class =  [1 0] , outlier class =  2\n",
      "iter :  6 , target class =  2 , outlier class =  [1 0]\n",
      "iter :  6 , target class =  [1 0] , outlier class =  2\n",
      "iter :  7 , target class =  2 , outlier class =  [1 0]\n",
      "iter :  7 , target class =  [1 0] , outlier class =  2\n",
      "iter :  8 , target class =  2 , outlier class =  [1 0]\n",
      "iter :  8 , target class =  [1 0] , outlier class =  2\n",
      "iter :  9 , target class =  2 , outlier class =  [1 0]\n",
      "iter :  9 , target class =  [1 0] , outlier class =  2\n",
      "iter :  0 , target class =  1 , outlier class =  [2 0]\n",
      "iter :  0 , target class =  [2 0] , outlier class =  1\n",
      "iter :  1 , target class =  1 , outlier class =  [2 0]\n",
      "iter :  1 , target class =  [2 0] , outlier class =  1\n",
      "iter :  2 , target class =  1 , outlier class =  [2 0]\n",
      "iter :  2 , target class =  [2 0] , outlier class =  1\n",
      "iter :  3 , target class =  1 , outlier class =  [2 0]\n",
      "iter :  3 , target class =  [2 0] , outlier class =  1\n",
      "iter :  4 , target class =  1 , outlier class =  [2 0]\n",
      "iter :  4 , target class =  [2 0] , outlier class =  1\n",
      "iter :  5 , target class =  1 , outlier class =  [2 0]\n",
      "iter :  5 , target class =  [2 0] , outlier class =  1\n",
      "iter :  6 , target class =  1 , outlier class =  [2 0]\n",
      "iter :  6 , target class =  [2 0] , outlier class =  1\n",
      "iter :  7 , target class =  1 , outlier class =  [2 0]\n",
      "iter :  7 , target class =  [2 0] , outlier class =  1\n",
      "iter :  8 , target class =  1 , outlier class =  [2 0]\n",
      "iter :  8 , target class =  [2 0] , outlier class =  1\n",
      "iter :  9 , target class =  1 , outlier class =  [2 0]\n",
      "iter :  9 , target class =  [2 0] , outlier class =  1\n",
      "iter :  0 , target class =  0 , outlier class =  [2 1]\n",
      "iter :  0 , target class =  [2 1] , outlier class =  0\n",
      "iter :  1 , target class =  0 , outlier class =  [2 1]\n",
      "iter :  1 , target class =  [2 1] , outlier class =  0\n",
      "iter :  2 , target class =  0 , outlier class =  [2 1]\n",
      "iter :  2 , target class =  [2 1] , outlier class =  0\n",
      "iter :  3 , target class =  0 , outlier class =  [2 1]\n",
      "iter :  3 , target class =  [2 1] , outlier class =  0\n",
      "iter :  4 , target class =  0 , outlier class =  [2 1]\n",
      "iter :  4 , target class =  [2 1] , outlier class =  0\n",
      "iter :  5 , target class =  0 , outlier class =  [2 1]\n",
      "iter :  5 , target class =  [2 1] , outlier class =  0\n",
      "iter :  6 , target class =  0 , outlier class =  [2 1]\n",
      "iter :  6 , target class =  [2 1] , outlier class =  0\n",
      "iter :  7 , target class =  0 , outlier class =  [2 1]\n",
      "iter :  7 , target class =  [2 1] , outlier class =  0\n",
      "iter :  8 , target class =  0 , outlier class =  [2 1]\n",
      "iter :  8 , target class =  [2 1] , outlier class =  0\n",
      "iter :  9 , target class =  0 , outlier class =  [2 1]\n",
      "iter :  9 , target class =  [2 1] , outlier class =  0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('bmdatasets/waveform.data', header=None, sep=',')\n",
    "X_data = df.drop(21, axis=1)\n",
    "scaler = MinMaxScaler(feature_range =(-1,1), copy=False)\n",
    "scaler.fit(X_data)\n",
    "scaled = pd.DataFrame(scaler.transform(X_data))\n",
    "scaled['class'] = df[21]\n",
    "df = scaled\n",
    "classes = df['class'].unique()\n",
    "scores_Waveform3_macro = []\n",
    "scores_Waveform3_micro = []\n",
    "scores_Waveform3_weighted = []\n",
    "for c in classes:\n",
    "    for iter_ in range(10):\n",
    "        print('iter : ',iter_,', target class = ', c, ', outlier class = ' ,np.delete(classes, np.argwhere(classes == c)))\n",
    "        target1 = df[df['class']==c]\n",
    "        target_train1 = target1.sample(frac = 0.7)\n",
    "        target_val1 = target1.drop(target_train1.index)\n",
    "        target_train1 = target_train1.reset_index(drop=True)\n",
    "        target_val1 = target_val1.reset_index(drop=True)\n",
    "        outliers1 = df[df['class']!=c].reset_index(drop=True)\n",
    "        testset1 = pd.concat([target_val1, outliers1]).reset_index(drop=True)\n",
    "\n",
    "        y_true1 =  [1 if  i ==c else -1 for i in testset1['class']]\n",
    "        target_train1 = target_train1.drop('class', axis=1)\n",
    "        testset1 = testset1.drop('class', axis=1)\n",
    "\n",
    "        model1 = PP4.PseudoPoints(target_train1, 10)\n",
    "        opt_comb1 = model1.search_optimal_hyperparameters()\n",
    "\n",
    "        clf1 = OneClassSVM(nu = opt_comb1[0], gamma = opt_comb1[1]).fit(target_train1)\n",
    "        y_pred1 = clf1.predict(testset1)\n",
    "        \n",
    "        print('iter : ',iter_,', target class = ', np.delete(classes, np.argwhere(classes == c)), ', outlier class = ' , c)\n",
    "        target2 = df[df['class']!=c]\n",
    "        target_train2 = target2.sample(frac = 0.7)\n",
    "        target_val2 = target2.drop(target_train2.index)\n",
    "        target_train2 = target_train2.reset_index(drop=True)\n",
    "        target_val2 = target_val2.reset_index(drop=True)\n",
    "        outliers2 = df[df['class']==c].reset_index(drop=True)\n",
    "        testset2 = pd.concat([target_val2, outliers2]).reset_index(drop=True)\n",
    "\n",
    "        y_true2 =  [1 if i!=c else -1 for i in testset2['class']]\n",
    "        target_train2 = target_train2.drop('class', axis=1)\n",
    "        testset2 = testset2.drop('class', axis=1)\n",
    "\n",
    "        model2 = PP4.PseudoPoints(target_train2, 10)\n",
    "        opt_comb2 = model2.search_optimal_hyperparameters()\n",
    "\n",
    "        clf2 = OneClassSVM(nu = opt_comb2[0], gamma = opt_comb2[1]).fit(target_train2)\n",
    "        y_pred2 = clf2.predict(testset2)\n",
    "        \n",
    "\n",
    "        score1_macro = f1_score(y_true1, y_pred1, average='macro')\n",
    "        score1_micro = f1_score(y_true1, y_pred1, average='micro')\n",
    "        score1_weighted = f1_score(y_true1, y_pred1, average='weighted')\n",
    "\n",
    "        score2_macro = f1_score(y_true2, y_pred2, average='macro')\n",
    "        score2_micro = f1_score(y_true2, y_pred2, average='micro')\n",
    "        score2_weighted = f1_score(y_true2, y_pred2, average='weighted')\n",
    "\n",
    "        mean_score_macro = np.mean([score1_macro, score2_macro])\n",
    "        mean_score_micro = np.mean([score1_micro, score2_micro])\n",
    "        mean_score_weighted = np.mean([score1_weighted, score2_weighted])\n",
    "\n",
    "        scores_Waveform3_macro.append(mean_score_macro)\n",
    "        scores_Waveform3_micro.append(mean_score_micro)\n",
    "        scores_Waveform3_weighted.append(mean_score_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.Winequality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter :  0\n",
      "iter :  1\n",
      "iter :  2\n",
      "iter :  3\n",
      "iter :  4\n",
      "iter :  5\n",
      "iter :  6\n",
      "iter :  7\n",
      "iter :  8\n",
      "iter :  9\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('bmdatasets/winequality-red.csv', sep=';') #red wine\n",
    "X_data = df.drop('quality', axis=1)\n",
    "scaler = MinMaxScaler(feature_range =(-1,1))\n",
    "scaler.fit(X_data)\n",
    "scaled = pd.DataFrame(scaler.transform(X_data))\n",
    "scaled['class'] = df['quality']\n",
    "df = scaled\n",
    "classes = df['class'].unique()\n",
    "normal = [3,4,5]\n",
    "out = [6,7,8]\n",
    "indice = []\n",
    "outIndice = []\n",
    "\n",
    "for c in normal:\n",
    "    indice += df['class'].index[df['class'] == c].tolist()\n",
    "\n",
    "for c in out:\n",
    "    outIndice += df['class'].index[df['class'] == c].tolist()\n",
    "\n",
    "indice = np.sort(indice)\n",
    "outIndice = np.sort(outIndice)\n",
    "\n",
    "scores_Winequality_macro = []\n",
    "scores_Winequality_micro = []\n",
    "scores_Winequality_weighted = []\n",
    "\n",
    "for iter_ in range(10):\n",
    "    print('iter : ', iter_)\n",
    "\n",
    "    target1 = df.iloc[indice]\n",
    "    target_train1 = target1.sample(frac = 0.7)\n",
    "    target_val1 = target1.drop(target_train1.index)\n",
    "    target_train1 = target_train1.reset_index(drop=True)\n",
    "    target_val1 = target_val1.reset_index(drop=True)\n",
    "    outliers1 = df.iloc[outIndice].reset_index(drop=True)\n",
    "    testset1 = pd.concat([target_val1, outliers1]).reset_index(drop=True)\n",
    "\n",
    "    y_true1 =  [1 if  i in normal else -1 for i in testset1['class']]\n",
    "    target_train1 = target_train1.drop('class', axis=1)\n",
    "    testset1 = testset1.drop('class', axis=1)\n",
    "\n",
    "    model1 = PP4.PseudoPoints(target_train1, 10)\n",
    "    opt_comb1 = model1.search_optimal_hyperparameters()\n",
    "\n",
    "    clf1 = OneClassSVM(nu = opt_comb1[0], gamma = opt_comb1[1]).fit(target_train1)\n",
    "    y_pred1 = clf1.predict(testset1)\n",
    "    \n",
    "\n",
    "    target2 = df.iloc[outIndice]\n",
    "    target_train2 = target2.sample(frac = 0.7)\n",
    "    target_val2 = target2.drop(target_train2.index)\n",
    "    target_train2 = target_train2.reset_index(drop=True)\n",
    "    target_val2 = target_val2.reset_index(drop=True)\n",
    "    outliers2 = df.iloc[indice].reset_index(drop=True)\n",
    "    testset2 = pd.concat([target_val2, outliers2]).reset_index(drop=True)\n",
    "\n",
    "    y_true2 =  [1 if i in out else -1 for i in testset2['class']]\n",
    "    target_train2 = target_train2.drop('class', axis=1)\n",
    "    testset2 = testset2.drop('class', axis=1)\n",
    "\n",
    "    model2 = PP4.PseudoPoints(target_train2, 10)\n",
    "    opt_comb2 = model2.search_optimal_hyperparameters()\n",
    "\n",
    "    clf2 = OneClassSVM(nu = opt_comb2[0], gamma = opt_comb2[1]).fit(target_train2)\n",
    "    y_pred2 = clf2.predict(testset2)\n",
    "    \n",
    "\n",
    "    score1_macro = f1_score(y_true1, y_pred1, average='macro')\n",
    "    score1_micro = f1_score(y_true1, y_pred1, average='micro')\n",
    "    score1_weighted = f1_score(y_true1, y_pred1, average='weighted')\n",
    "\n",
    "    score2_macro = f1_score(y_true2, y_pred2, average='macro')\n",
    "    score2_micro = f1_score(y_true2, y_pred2, average='micro')\n",
    "    score2_weighted = f1_score(y_true2, y_pred2, average='weighted')\n",
    "\n",
    "    mean_score_macro = np.mean([score1_macro, score2_macro])\n",
    "    mean_score_micro = np.mean([score1_micro, score2_micro])\n",
    "    mean_score_weighted = np.mean([score1_weighted, score2_weighted])\n",
    "\n",
    "    scores_Winequality_macro.append(mean_score_macro)\n",
    "    scores_Winequality_micro.append(mean_score_micro)\n",
    "    scores_Winequality_weighted.append(mean_score_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.SVMguide1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('svmguide1.txt', header = None, sep = ' ')\n",
    "classes = df[0]\n",
    "df = df.drop(0, axis=1)\n",
    "val = []\n",
    "for j in range(df.shape[0]):\n",
    "    for i in range(1,5):\n",
    "        val.append(str(df[i][j]).split(':')[1])\n",
    "\n",
    "val = np.array(val, dtype=float)\n",
    "val = val.reshape(df.shape)\n",
    "df = pd.DataFrame(val)\n",
    "df['class'] = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter :  0\n",
      "iter :  1\n",
      "iter :  2\n",
      "iter :  3\n",
      "iter :  4\n",
      "iter :  5\n",
      "iter :  6\n",
      "iter :  7\n",
      "iter :  8\n",
      "iter :  9\n",
      "iter :  10\n",
      "iter :  11\n",
      "iter :  12\n",
      "iter :  13\n",
      "iter :  14\n",
      "iter :  15\n",
      "iter :  16\n",
      "iter :  17\n",
      "iter :  18\n",
      "iter :  19\n"
     ]
    }
   ],
   "source": [
    "X_data = df.drop('class', axis=1)\n",
    "scaler = MinMaxScaler(feature_range =(-1,1), copy=False)\n",
    "scaler.fit(X_data)\n",
    "scaled = pd.DataFrame(scaler.transform(X_data))\n",
    "scaled['class'] = df['class']\n",
    "df = scaled\n",
    "classes = df['class'].unique()\n",
    "scores_SVMguide1_macro = []\n",
    "scores_SVMguide1_micro = []\n",
    "scores_SVMguide1_weighted = []\n",
    "\n",
    "\n",
    "for iter_ in range(20):\n",
    "    print('iter : ',iter_)\n",
    "    target1 = df[df['class']==1]\n",
    "    target_train1 = target1.sample(frac = 0.7)\n",
    "    target_val1 = target1.drop(target_train1.index)\n",
    "    target_train1 = target_train1.reset_index(drop=True)\n",
    "    target_val1 = target_val1.reset_index(drop=True)\n",
    "    outliers1 = df[df['class']!=1].reset_index(drop=True)\n",
    "    testset1 = pd.concat([target_val1, outliers1]).reset_index(drop=True)\n",
    "\n",
    "    y_true1 =  [1 if  i ==1 else -1 for i in testset1['class']]\n",
    "    target_train1 = target_train1.drop('class', axis=1)\n",
    "    testset1 = testset1.drop('class', axis=1)\n",
    "\n",
    "    model1 = PP4.PseudoPoints(target_train1, 10)\n",
    "    opt_comb1 = model1.search_optimal_hyperparameters()\n",
    "\n",
    "    clf1 = OneClassSVM(nu = opt_comb1[0], gamma = opt_comb1[1]).fit(target_train1)\n",
    "    y_pred1 = clf1.predict(testset1)\n",
    "    \n",
    "\n",
    "    \n",
    "    target2 = df[df['class']!=1]\n",
    "    target_train2 = target2.sample(frac = 0.7)\n",
    "    target_val2 = target2.drop(target_train2.index)\n",
    "    target_train2 = target_train2.reset_index(drop=True)\n",
    "    target_val2 = target_val2.reset_index(drop=True)\n",
    "    outliers2 = df[df['class']==1].reset_index(drop=True)\n",
    "    testset2 = pd.concat([target_val2, outliers2]).reset_index(drop=True)\n",
    "\n",
    "    y_true2 =  [1 if i!=1 else -1 for i in testset2['class']]\n",
    "    target_train2 = target_train2.drop('class', axis=1)\n",
    "    testset2 = testset2.drop('class', axis=1)\n",
    "\n",
    "    model2 = PP4.PseudoPoints(target_train2, 10)\n",
    "    opt_comb2 = model2.search_optimal_hyperparameters()\n",
    "\n",
    "    clf2 = OneClassSVM(nu = opt_comb2[0], gamma = opt_comb2[1]).fit(target_train2)\n",
    "    y_pred2 = clf2.predict(testset2)\n",
    "    \n",
    "\n",
    "    score1_macro = f1_score(y_true1, y_pred1, average='macro')\n",
    "    score1_micro = f1_score(y_true1, y_pred1, average='micro')\n",
    "    score1_weighted = f1_score(y_true1, y_pred1, average='weighted')\n",
    "\n",
    "    score2_macro = f1_score(y_true2, y_pred2, average='macro')\n",
    "    score2_micro = f1_score(y_true2, y_pred2, average='micro')\n",
    "    score2_weighted = f1_score(y_true2, y_pred2, average='weighted')\n",
    "\n",
    "    mean_score_macro = np.mean([score1_macro, score2_macro])\n",
    "    mean_score_micro = np.mean([score1_micro, score2_micro])\n",
    "    mean_score_weighted = np.mean([score1_weighted, score2_weighted])\n",
    "\n",
    "    scores_SVMguide1_macro.append(mean_score_macro)\n",
    "    scores_SVMguide1_micro.append(mean_score_micro)\n",
    "    scores_SVMguide1_weighted.append(mean_score_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter :  0\n",
      "iter :  1\n",
      "iter :  2\n",
      "iter :  3\n",
      "iter :  4\n",
      "iter :  5\n",
      "iter :  6\n",
      "iter :  7\n",
      "iter :  8\n",
      "iter :  9\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('bmdatasets/diabetes.txt', header = None, sep = ' ')\n",
    "df = df.drop(1, axis=1)\n",
    "df.columns = range(df.shape[1])\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "classes = df[0]\n",
    "df = df.drop(0, axis=1)\n",
    "val = []\n",
    "for j in range(df.shape[0]):\n",
    "    for i in range(1,9):\n",
    "        val.append(str(df[i][j]).split(':')[1])\n",
    "        \n",
    "val = np.array(val, dtype=float)\n",
    "val = val.reshape(df.shape)\n",
    "df = pd.DataFrame(val)\n",
    "df['class'] = classes\n",
    "\n",
    "X_data = df.drop('class', axis=1)\n",
    "scaler = MinMaxScaler(feature_range =(-1,1))\n",
    "scaler.fit(X_data)\n",
    "scaled = pd.DataFrame(scaler.transform(X_data))\n",
    "scaled['class'] = df['class']\n",
    "df = scaled\n",
    "classes = df['class'].unique()\n",
    "scores_Diabetes_macro = []\n",
    "scores_Diabetes_micro = []\n",
    "scores_Diabetes_weighted = []\n",
    "\n",
    "for iter_ in range(10):\n",
    "    print('iter : ',iter_)\n",
    "    target1 = df[df['class']==1]\n",
    "    target_train1 = target1.sample(frac = 0.7)\n",
    "    target_val1 = target1.drop(target_train1.index)\n",
    "    target_train1 = target_train1.reset_index(drop=True)\n",
    "    target_val1 = target_val1.reset_index(drop=True)\n",
    "    outliers1 = df[df['class']!=1].reset_index(drop=True)\n",
    "    testset1 = pd.concat([target_val1, outliers1]).reset_index(drop=True)\n",
    "\n",
    "    y_true1 =  [1 if  i ==1 else -1 for i in testset1['class']]\n",
    "    target_train1 = target_train1.drop('class', axis=1)\n",
    "    testset1 = testset1.drop('class', axis=1)\n",
    "\n",
    "    model1 = PP4.PseudoPoints(target_train1, 10)\n",
    "    opt_comb1 = model1.search_optimal_hyperparameters()\n",
    "\n",
    "    clf1 = OneClassSVM(nu = opt_comb1[0], gamma = opt_comb1[1]).fit(target_train1)\n",
    "    y_pred1 = clf1.predict(testset1)\n",
    "    \n",
    "\n",
    "    \n",
    "    target2 = df[df['class']!=1]\n",
    "    target_train2 = target2.sample(frac = 0.7)\n",
    "    target_val2 = target2.drop(target_train2.index)\n",
    "    target_train2 = target_train2.reset_index(drop=True)\n",
    "    target_val2 = target_val2.reset_index(drop=True)\n",
    "    outliers2 = df[df['class']==1].reset_index(drop=True)\n",
    "    testset2 = pd.concat([target_val2, outliers2]).reset_index(drop=True)\n",
    "\n",
    "    y_true2 =  [1 if i!=1 else -1 for i in testset2['class']]\n",
    "    target_train2 = target_train2.drop('class', axis=1)\n",
    "    testset2 = testset2.drop('class', axis=1)\n",
    "\n",
    "    model2 = PP4.PseudoPoints(target_train2, 10)\n",
    "    opt_comb2 = model2.search_optimal_hyperparameters()\n",
    "\n",
    "    clf2 = OneClassSVM(nu = opt_comb2[0], gamma = opt_comb2[1]).fit(target_train2)\n",
    "    y_pred2 = clf2.predict(testset2)\n",
    "    \n",
    "    score1_macro = f1_score(y_true1, y_pred1, average='macro')\n",
    "    score1_micro = f1_score(y_true1, y_pred1, average='micro')\n",
    "    score1_weighted = f1_score(y_true1, y_pred1, average='weighted')\n",
    "\n",
    "    score2_macro = f1_score(y_true2, y_pred2, average='macro')\n",
    "    score2_micro = f1_score(y_true2, y_pred2, average='micro')\n",
    "    score2_weighted = f1_score(y_true2, y_pred2, average='weighted')\n",
    "\n",
    "    mean_score_macro = np.mean([score1_macro, score2_macro])\n",
    "    mean_score_micro = np.mean([score1_micro, score2_micro])\n",
    "    mean_score_weighted = np.mean([score1_weighted, score2_weighted])\n",
    "\n",
    "    scores_Diabetes_macro.append(mean_score_macro)\n",
    "    scores_Diabetes_micro.append(mean_score_micro)\n",
    "    scores_Diabetes_weighted.append(mean_score_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.vowel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter :  0\n",
      "iter :  1\n",
      "iter :  2\n",
      "iter :  3\n",
      "iter :  4\n",
      "iter :  5\n",
      "iter :  6\n",
      "iter :  7\n",
      "iter :  8\n",
      "iter :  9\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('bmdatasets/vowel.txt', sep = ' ', header = None)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "classes = df[0]\n",
    "df = df.drop(0, axis=1)\n",
    "val = []\n",
    "for j in range(df.shape[0]):\n",
    "    for i in range(1,11):\n",
    "        val.append(str(df[i][j]).split(':')[1])\n",
    "        \n",
    "val = np.array(val, dtype=float)\n",
    "val = val.reshape(df.shape)\n",
    "df = pd.DataFrame(val)\n",
    "df['class'] = classes\n",
    "X_data = df.drop('class', axis=1)\n",
    "scaler = MinMaxScaler(feature_range =(-1,1))\n",
    "scaler.fit(X_data)\n",
    "scaled = pd.DataFrame(scaler.transform(X_data))\n",
    "scaled['class'] = df['class']\n",
    "df = scaled\n",
    "\n",
    "normal = [0,1,2,3,4,5]\n",
    "out = [6,7,8,9,10]\n",
    "indice = []\n",
    "outIndice = []\n",
    "\n",
    "for c in normal:\n",
    "    indice += df['class'].index[df['class'] == c].tolist()\n",
    "\n",
    "for c in out:\n",
    "    outIndice += df['class'].index[df['class'] == c].tolist()\n",
    "\n",
    "indice = np.sort(indice)\n",
    "outIndice = np.sort(outIndice)\n",
    "\n",
    "scores_Vowel_macro = []\n",
    "scores_Vowel_micro = []\n",
    "scores_Vowel_weighted = []\n",
    "\n",
    "for iter_ in range(10):\n",
    "    print('iter : ', iter_)\n",
    "\n",
    "    target1 = df.iloc[indice]\n",
    "    target_train1 = target1.sample(frac = 0.7)\n",
    "    target_val1 = target1.drop(target_train1.index)\n",
    "    target_train1 = target_train1.reset_index(drop=True)\n",
    "    target_val1 = target_val1.reset_index(drop=True)\n",
    "    outliers1 = df.iloc[outIndice].reset_index(drop=True)\n",
    "    testset1 = pd.concat([target_val1, outliers1]).reset_index(drop=True)\n",
    "\n",
    "    y_true1 =  [1 if  i in normal else -1 for i in testset1['class']]\n",
    "    target_train1 = target_train1.drop('class', axis=1)\n",
    "    testset1 = testset1.drop('class', axis=1)\n",
    "\n",
    "    model1 = PP4.PseudoPoints(target_train1, 10)\n",
    "    opt_comb1 = model1.search_optimal_hyperparameters()\n",
    "\n",
    "    clf1 = OneClassSVM(nu = opt_comb1[0], gamma = opt_comb1[1]).fit(target_train1)\n",
    "    y_pred1 = clf1.predict(testset1)\n",
    "   \n",
    "\n",
    "    target2 = df.iloc[outIndice]\n",
    "    target_train2 = target2.sample(frac = 0.7)\n",
    "    target_val2 = target2.drop(target_train2.index)\n",
    "    target_train2 = target_train2.reset_index(drop=True)\n",
    "    target_val2 = target_val2.reset_index(drop=True)\n",
    "    outliers2 = df.iloc[indice].reset_index(drop=True)\n",
    "    testset2 = pd.concat([target_val2, outliers2]).reset_index(drop=True)\n",
    "\n",
    "    y_true2 =  [1 if i in out else -1 for i in testset2['class']]\n",
    "    target_train2 = target_train2.drop('class', axis=1)\n",
    "    testset2 = testset2.drop('class', axis=1)\n",
    "\n",
    "    model2 = PP4.PseudoPoints(target_train2, 10)\n",
    "    opt_comb2 = model2.search_optimal_hyperparameters()\n",
    "\n",
    "    clf2 = OneClassSVM(nu = opt_comb2[0], gamma = opt_comb2[1]).fit(target_train2)\n",
    "    y_pred2 = clf2.predict(testset2)\n",
    "    \n",
    "\n",
    "    score1_macro = f1_score(y_true1, y_pred1, average='macro')\n",
    "    score1_micro = f1_score(y_true1, y_pred1, average='micro')\n",
    "    score1_weighted = f1_score(y_true1, y_pred1, average='weighted')\n",
    "\n",
    "    score2_macro = f1_score(y_true2, y_pred2, average='macro')\n",
    "    score2_micro = f1_score(y_true2, y_pred2, average='micro')\n",
    "    score2_weighted = f1_score(y_true2, y_pred2, average='weighted')\n",
    "\n",
    "    mean_score_macro = np.mean([score1_macro, score2_macro])\n",
    "    mean_score_micro = np.mean([score1_micro, score2_micro])\n",
    "    mean_score_weighted = np.mean([score1_weighted, score2_weighted])\n",
    "\n",
    "    scores_Vowel_macro.append(mean_score_macro)\n",
    "    scores_Vowel_micro.append(mean_score_micro)\n",
    "    scores_Vowel_weighted.append(mean_score_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_macro = {'Abalone':scores_Abalone_macro,'Australian':scores_Australian_macro, 'Balance' : scores_Balance_macro, 'Glass' : scores_Glass_macro, 'Heart' : scores_Heart_macro, 'Landsat' : scores_Landsat_macro, \n",
    "            'Letter':scores_Letter_macro, 'Segment':scores_Segment_macro, 'Sonar':scores_Sonar_macro, 'Vehicle':scores_Vehicle_macro, 'Waveform3':scores_Waveform3_macro, 'Winequality':scores_Winequality_macro,\n",
    "           'SVMguide1':scores_SVMguide1_macro, 'Diabetes':scores_Diabetes_macro, 'Vowel':scores_Vowel_macro}\n",
    "f1_macro_mean = {} \n",
    "for k in f1_macro.keys():\n",
    "    scores = f1_macro[k]\n",
    "    f1_macro_mean[k] = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro = {'Abalone':scores_Abalone_micro,'Australian':scores_Australian_micro, 'Balance' : scores_Balance_micro, 'Glass' : scores_Glass_micro, 'Heart' : scores_Heart_micro, 'Landsat' : scores_Landsat_micro, \n",
    "            'Letter':scores_Letter_micro, 'Segment':scores_Segment_micro, 'Sonar':scores_Sonar_micro, 'Vehicle':scores_Vehicle_micro, 'Waveform3':scores_Waveform3_micro, 'Winequality':scores_Winequality_micro,\n",
    "           'SVMguide1':scores_SVMguide1_micro, 'Diabetes':scores_Diabetes_micro, 'Vowel':scores_Vowel_micro}\n",
    "f1_micro_mean = {} \n",
    "for k in f1_micro.keys():\n",
    "    scores = f1_micro[k]\n",
    "    f1_micro_mean[k] = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_weighted = {'Abalone':scores_Abalone_weighted,'Australian':scores_Australian_weighted, 'Balance' : scores_Balance_weighted, 'Glass' : scores_Glass_weighted, 'Heart' : scores_Heart_weighted, 'Landsat' : scores_Landsat_weighted, \n",
    "            'Letter':scores_Letter_weighted, 'Segment':scores_Segment_weighted, 'Sonar':scores_Sonar_weighted, 'Vehicle':scores_Vehicle_weighted, 'Waveform3':scores_Waveform3_weighted, 'Winequality':scores_Winequality_weighted,\n",
    "           'SVMguide1':scores_SVMguide1_weighted, 'Diabetes':scores_Diabetes_weighted, 'Vowel':scores_Vowel_weighted}\n",
    "f1_weighted_mean = {} \n",
    "for k in f1_weighted.keys():\n",
    "    scores = f1_weighted[k]\n",
    "    f1_weighted_mean[k] = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abalone': 0.42956020115846005,\n",
       " 'Australian': 0.5825646256076714,\n",
       " 'Balance': 0.7260307428564896,\n",
       " 'Glass': 0.6275776284712566,\n",
       " 'Heart': 0.6076398701776257,\n",
       " 'Landsat': 0.8098854323380793,\n",
       " 'Letter': 0.6971528174420796,\n",
       " 'Segment': 0.7750995541796865,\n",
       " 'Sonar': 0.5831122286663195,\n",
       " 'Vehicle': 0.5666267932696067,\n",
       " 'Waveform3': 0.6466377250450311,\n",
       " 'Winequality': 0.4482553991235784,\n",
       " 'SVMguide1': 0.8469852112809765,\n",
       " 'Diabetes': 0.4591368007167511,\n",
       " 'Vowel': 0.7748490752301298}"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_macro_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abalone': 0.45779301761920466,\n",
       " 'Australian': 0.6356974049801933,\n",
       " 'Balance': 0.8232677255129409,\n",
       " 'Glass': 0.7098045426510994,\n",
       " 'Heart': 0.6726825939068124,\n",
       " 'Landsat': 0.8631813974769649,\n",
       " 'Letter': 0.8091572815277898,\n",
       " 'Segment': 0.8414636374035631,\n",
       " 'Sonar': 0.6589960432953669,\n",
       " 'Vehicle': 0.632105805341449,\n",
       " 'Waveform3': 0.7124674083496176,\n",
       " 'Winequality': 0.47447219734854695,\n",
       " 'SVMguide1': 0.8701342885544914,\n",
       " 'Diabetes': 0.503568297153782,\n",
       " 'Vowel': 0.8358397024846921}"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_weighted_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abalone': 0.46547940377568325,\n",
       " 'Australian': 0.6168855076078822,\n",
       " 'Balance': 0.8124085230170507,\n",
       " 'Glass': 0.6857977528089887,\n",
       " 'Heart': 0.6454594330400784,\n",
       " 'Landsat': 0.8599991344306301,\n",
       " 'Letter': 0.8399547478186579,\n",
       " 'Segment': 0.8452741702741703,\n",
       " 'Sonar': 0.6395467032967034,\n",
       " 'Vehicle': 0.601234332983435,\n",
       " 'Waveform3': 0.6989853949735807,\n",
       " 'Winequality': 0.47203867561010415,\n",
       " 'SVMguide1': 0.8969848825904772,\n",
       " 'Diabetes': 0.48454957927734704,\n",
       " 'Vowel': 0.830586162440911}"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_micro_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_micro :  {'Abalone': [0.3526222362374215, 0.36822411338123784, 0.34282987009046406, 0.35504618526951137, 0.38034995478389444, 0.364044030940646, 0.3519765194918339, 0.3781532986004552, 0.3691433989958658, 0.3703221905571702, 0.4002636589057491, 0.3829118882083977, 0.38930341786557043, 0.38805172386487813, 0.40325050169569754, 0.39756640822079586, 0.37755415146119575, 0.3904452422053344, 0.3972697459960498, 0.39787721068504367, 0.6410524305675844, 0.6416120752565789, 0.6608447564785139, 0.6372956712455009, 0.5457869345074887, 0.6626687418866307, 0.6362945353385906, 0.6612924440644006, 0.6741784953922962, 0.6461502810757007], 'Australian': [0.5662060364180594, 0.5410800698428535, 0.6406784734347717, 0.5084435021202295, 0.7448216512846096, 0.7588924918932403, 0.5659416313295086, 0.5100249438762784, 0.5022200049887753, 0.6526490396607634, 0.5814816662509354, 0.6577650286854577, 0.6168645547518083, 0.7292866051384386, 0.5749064604639561, 0.5676702419555999, 0.6767223746570217, 0.5943576951858318, 0.7149064604639561, 0.6327912197555501], 'Balance': [0.7797480221338089, 0.5217107970915078, 0.5259294827822747, 0.806797914665935, 0.5031211414460145, 0.5515845795033613, 0.8259477751863539, 0.5301481684730416, 0.8098961906068505, 0.3401243883477386, 0.9062395546561164, 0.9124596619810753, 0.9134350671844518, 0.9083969929564197, 0.8986307863406808, 0.91214972014075, 0.9234079016937411, 0.9018669437911357, 0.9150303560684789, 0.9117364643536497, 0.9115875707244738, 0.9198618024029608, 0.9145137863346036, 0.9044376378785393, 0.8975976468729299, 0.8907120761849199, 0.9198618024029608, 0.9086036208499699, 0.8926173069092721, 0.9141005305475032], 'Glass': [0.8269101123595506, 0.8266853932584269, 0.7102808988764044, 0.6687078651685393, 0.5235955056179776, 0.5335955056179775, 0.7055056179775281, 0.739943820224719, 0.7826404494382022, 0.5401123595505618], 'Heart': [0.6465298142717497, 0.6679863147605083, 0.5900293255131965, 0.6488758553274683, 0.6532746823069404, 0.6670576735092864, 0.6516129032258065, 0.640811339198436, 0.6337243401759531, 0.5651515151515152, 0.6583088954056696, 0.5611436950146628, 0.705425219941349, 0.6495112414467252, 0.6771260997067449, 0.6791788856304986, 0.6929618768328446, 0.6572825024437927, 0.6347996089931573, 0.6283968719452591], 'Landsat': [0.8500893137161324, 0.8636917956473718, 0.8639926699317209, 0.8612782799595198, 0.8658011763443448, 0.853574416140853, 0.872230696765557, 0.853773912094909, 0.8557175896145904, 0.8598414940913033], 'Letter': [0.8400439449523827, 0.8393147677629528, 0.8401587979041544, 0.8404980312748433, 0.8375063160247667, 0.8411176809971277, 0.8402022656931144, 0.8413113747901877, 0.8415822052694557, 0.8378120935175929], 'Segment': [0.8545660688517831, 0.847041847041847, 0.8446196660482375, 0.8454442383013812, 0.8451865594722737, 0.8400845186559472, 0.8449804164089878, 0.8546176046176046, 0.8340032982890127, 0.8421974850546279], 'Sonar': [0.5414835164835166, 0.5678571428571428, 0.7024725274725274, 0.6686813186813186, 0.6766483516483517, 0.6975274725274725, 0.6538461538461539, 0.5596153846153846, 0.5793956043956043, 0.6087912087912088, 0.6835164835164835, 0.6832417582417581, 0.7277472527472527, 0.6126373626373627, 0.665934065934066, 0.5755494505494506, 0.6752747252747253, 0.61510989010989, 0.6005494505494506, 0.695054945054945], 'Vehicle': [0.6123634036894072, 0.6001685367414278, 0.603190070133031, 0.6010864974050361, 0.6082649916149835, 0.6042418564970287, 0.5919215279549341, 0.5949346972402632, 0.6031817060267568, 0.5929900425314805], 'Waveform3': [0.7148266888265723, 0.6932401551466683, 0.7237074876841425, 0.6950947686361986, 0.6916041247642508, 0.7235250666851722, 0.7116721427127594, 0.7237323765844834, 0.7047147190321321, 0.7097250986796098, 0.7201254133438552, 0.6816330071914898, 0.6844664132428371, 0.7089469905765097, 0.7033579261638956, 0.7162841777522854, 0.7174477966147862, 0.7108122492734976, 0.7100454032792771, 0.7284434363753958, 0.6871485354010025, 0.685280584273183, 0.6693667763157896, 0.7026482221177945, 0.6908364661654136, 0.6632048872180452, 0.687768248746867, 0.6780251409774436, 0.672609257518797, 0.6592682879072682], 'Winequality': [0.5778150421007564, 0.40862708719851576, 0.5575852718709862, 0.5535179106607677, 0.39899386327957753, 0.41133866133866137, 0.4091622663051234, 0.41490652204937917, 0.4141215926930213, 0.5743185386042529], 'SVMguide1': [0.9119291820503407, 0.914612690166636, 0.8317853610777591, 0.9101957279120719, 0.904332312292462, 0.9093652575895548, 0.908199953031611, 0.9123110864480424, 0.9064762945757618, 0.9029572274707574, 0.9021408781969227, 0.8318281058737711, 0.903630458007945, 0.8286576124029114, 0.9111508705562905, 0.9156152846230939, 0.9083481604344499, 0.9058218921034842, 0.9143499877744794, 0.9059893092211975], 'Diabetes': [0.47589506682065663, 0.5077709948853324, 0.44522768520046196, 0.5239481933674311, 0.5139910905791123, 0.44834185777924435, 0.5188830226035308, 0.47151047681900676, 0.5035472694274872, 0.43638013529120606], 'Vowel': [0.8202707348517405, 0.8332660077352815, 0.8276794155565106, 0.8350880962612806, 0.8364847443059733, 0.8307563386334336, 0.8218091963902021, 0.8303308981521271, 0.8408165019338203, 0.8293596905887408]}  f1_macro :  {'Abalone': [0.3464677995607596, 0.3596751760175403, 0.3324014204181646, 0.34764820802280644, 0.37198973487903786, 0.3570494682091101, 0.3456715932586013, 0.3687316933555904, 0.3619685775776712, 0.36323603312690556, 0.38681228189394434, 0.3631619021071715, 0.3767649857032573, 0.36523415099002027, 0.3893941354392991, 0.3827173838228145, 0.3618144001113036, 0.3783495606985645, 0.3827770908406974, 0.3824624358448445, 0.5501568825450354, 0.5492453806863146, 0.5715177288274484, 0.5437301720491201, 0.4941390263167179, 0.5804212282882435, 0.5415699335621467, 0.5781800505680743, 0.5968019861013845, 0.5567156139312124], 'Australian': [0.553434231058616, 0.5313460347128351, 0.6159514886089972, 0.5022836275799045, 0.6849576349202895, 0.6855412164772686, 0.5439130170014519, 0.5062432368245907, 0.4888433868471493, 0.6165231706334281, 0.5620951129951937, 0.5770742032633274, 0.5934360615977836, 0.6583365640984098, 0.5582764474552133, 0.5525097543109623, 0.614037867277925, 0.5732432813899042, 0.6272664698846496, 0.6059797052155291], 'Balance': [0.5282352638846457, 0.4091990602206788, 0.3804436791180491, 0.5447331652035684, 0.4032662063514561, 0.4257838732563292, 0.5357737992922489, 0.3854983633679115, 0.577896813942453, 0.28147020101938686, 0.8587008383516639, 0.8704800947160294, 0.871708789182457, 0.8661545388188165, 0.8465291054123542, 0.8725150282998595, 0.8877896523290956, 0.8528635409858127, 0.8713625382684762, 0.875922194990238, 0.8701644897933456, 0.884596420203642, 0.8758707937415704, 0.8607307564605836, 0.8479011595542387, 0.8379095690715785, 0.8847324940505912, 0.8584049001140455, 0.8374063139033663, 0.8768786417901948], 'Glass': [0.7184626312050191, 0.7359207082227724, 0.6598613428122952, 0.6143531729280991, 0.5179040495383768, 0.5294532750445313, 0.6138881284788757, 0.6713209332470679, 0.6938313269660915, 0.520780716269438], 'Heart': [0.6041887649389183, 0.6161386340143715, 0.5675943680247773, 0.6031548516842634, 0.6091957043704863, 0.614685234454875, 0.6172797626888975, 0.5983921754124384, 0.5931755722100821, 0.5473353101924531, 0.6131456133536117, 0.5446302444209666, 0.6630798769849158, 0.6185866731466203, 0.6375007673066004, 0.6338264685631796, 0.6476984126984127, 0.6211393858250609, 0.6012643153947502, 0.6007852678668332], 'Landsat': [0.7981503513636674, 0.8113590675578226, 0.8156307458940197, 0.806089753008525, 0.8210003226073395, 0.8028812353618092, 0.8249284104359437, 0.7997137945509105, 0.8053465924744418, 0.8137540501263139], 'Letter': [0.6979764739200287, 0.6957825848603216, 0.6978103003404945, 0.6986392500128507, 0.6907809204081565, 0.7000291048399515, 0.6975359627913196, 0.7001188251432715, 0.7011680528067308, 0.6916866992976701], 'Segment': [0.7853953604211624, 0.7737087445661299, 0.7766086595220073, 0.7742599246208162, 0.7742516360806588, 0.7662905950620671, 0.7789825554624793, 0.7925781265517671, 0.7524071006276773, 0.7765128388820992], 'Sonar': [0.5097537921085971, 0.5320901320901321, 0.6411596034711613, 0.599719427244582, 0.601303459443927, 0.6096546983387505, 0.5689962958004195, 0.5216816233801977, 0.518634715964994, 0.5716789205184893, 0.6152935606060606, 0.6177007660219695, 0.6580049202681548, 0.5811922168103834, 0.6117652020672273, 0.5352305665349143, 0.6152052174250036, 0.5589111781076066, 0.5593114766778873, 0.634956800445931], 'Vehicle': [0.5752120195852984, 0.5630624881046711, 0.5664763518359262, 0.5698946026323815, 0.5720827888296189, 0.5729042256021477, 0.5589915872312812, 0.5617055595537326, 0.5700105785558374, 0.5559277307651719], 'Waveform3': [0.660860066614759, 0.6464273210837441, 0.6746569868343577, 0.6519042491432903, 0.6438536850042268, 0.6727897555157247, 0.6634442228781058, 0.6692482735511273, 0.6563832837477594, 0.6539017206401397, 0.6664562367061596, 0.6276327038913839, 0.6332635358741143, 0.6520597915044394, 0.6441584278558445, 0.6604380228839853, 0.6626213841639431, 0.654028565983777, 0.6515396938797122, 0.6680636433153313, 0.6286313100551095, 0.631166800951389, 0.6215997528625017, 0.6441663669679156, 0.643753899601949, 0.6171508900216264, 0.6320610270319851, 0.6313777348749636, 0.622016851012293, 0.6134755468992759], 'Winequality': [0.5113029606855254, 0.4086128098960569, 0.49983102674719987, 0.49592814957971365, 0.39896156900220714, 0.4110318103935978, 0.4089733611999623, 0.4145666825940514, 0.413834888404977, 0.5195107327324928], 'SVMguide1': [0.8772718179574653, 0.8780895915045722, 0.7438372289488371, 0.8705823537023205, 0.8231300166396309, 0.8750779618192186, 0.8686890533658997, 0.8752278680336285, 0.8676746643171096, 0.8600634110673144, 0.8625947744262499, 0.7459934165063933, 0.8606729432213223, 0.6969087392503633, 0.8727520515988363, 0.8800483457494989, 0.8733752637200887, 0.8639535848932157, 0.8786431241524617, 0.8651180147450999], 'Diabetes': [0.4576604809863848, 0.4945758622957527, 0.4416584284979731, 0.464944262668594, 0.45011405152710643, 0.44260359386864806, 0.45208332228897047, 0.46594634496224224, 0.4913160027411255, 0.4304656573307145], 'Vowel': [0.7562658811153591, 0.7710857824495931, 0.7712463558931751, 0.7845632923196555, 0.7850445306327659, 0.7771646000741359, 0.764559455410533, 0.7762055157586357, 0.7891946728504601, 0.7731606657969843]} f1_weighted :  {'Abalone': [0.3390643316837625, 0.36105952308433875, 0.3200147209775577, 0.3385666383964434, 0.38090315273693887, 0.3586987790659827, 0.3354676985548356, 0.3768094465419003, 0.36545838201917846, 0.36751304117314265, 0.4065426639707911, 0.37472211290611745, 0.3925503793213672, 0.3790035677663446, 0.41142226435090556, 0.4082137863966827, 0.37385017203305315, 0.393288974177862, 0.4044498164037268, 0.402945661273341, 0.6151252433092627, 0.6179185119922512, 0.6452071448032254, 0.61210545251927, 0.5491849698767296, 0.6521558551253379, 0.6071287600144721, 0.6496822148973344, 0.6667098656691406, 0.6280273975348427], 'Australian': [0.5942436938151352, 0.5684550242313884, 0.6631329818793761, 0.5298566431462783, 0.7573848600939472, 0.7643232988046853, 0.5981082802123245, 0.5294593726932931, 0.5088657109847393, 0.672684229497493, 0.6116306460670973, 0.6569340226187466, 0.6427946845174777, 0.7373825033686083, 0.6030093031579422, 0.5937103557700563, 0.6796661683746117, 0.6218786734184356, 0.7199574392640592, 0.660470207688171], 'Balance': [0.7969624755124729, 0.5950571958933434, 0.5881570858953604, 0.8155690496704322, 0.5815076758515034, 0.6237193353244158, 0.8148558553706127, 0.5882591397802983, 0.8180944025108289, 0.34448268961886785, 0.9032177514624131, 0.9106300214741377, 0.9111117847513062, 0.9065436369740145, 0.8948850516596969, 0.910834007764242, 0.9220631455869195, 0.8979566100436787, 0.9123372856599892, 0.9112028277397403, 0.9104380432073729, 0.9192102965913433, 0.9130710609000243, 0.9023759050906677, 0.8932922914251948, 0.8871429710528862, 0.919364822301739, 0.9040691813630488, 0.8889588811309619, 0.9126612837807182], 'Glass': [0.8077178697291495, 0.8441951952665305, 0.7575149512228552, 0.707097543393679, 0.5430445909064165, 0.5544787997882643, 0.7265603090579699, 0.7850624120556591, 0.8001668898296218, 0.5722068652608486], 'Heart': [0.6752138238667746, 0.6909846245762313, 0.6210729047132784, 0.6771568090353668, 0.6809841564064965, 0.6926740040528028, 0.6806583697961166, 0.6677987646834049, 0.6632077729867548, 0.5967397681683396, 0.6833764836778232, 0.5890390929735075, 0.7266604230376423, 0.6773057110452008, 0.703177805910853, 0.7057739487393259, 0.7168449471675278, 0.6849890873671638, 0.663288392511904, 0.6567049874197337], 'Landsat': [0.8542149097463203, 0.8657247946223422, 0.8670478836933206, 0.8626594645213393, 0.8697737985325137, 0.857460517361142, 0.8742325208882937, 0.8565999670319083, 0.8594615291991283, 0.8646385891733411], 'Letter': [0.8095422419418419, 0.8083315248262548, 0.8095158334584276, 0.8099859631211972, 0.8055053988056876, 0.8108243983856803, 0.8094262184482617, 0.8109420659667287, 0.8114945779732969, 0.80600459235052], 'Segment': [0.8503788793799598, 0.8422987635696828, 0.8414249134708516, 0.8412955364918373, 0.8412327097287785, 0.8357109370675837, 0.8421855307082877, 0.8521556470582008, 0.8280249822196665, 0.8399284743407813], 'Sonar': [0.5607860767794381, 0.5940825840825841, 0.7228783235547828, 0.6880414767121423, 0.6959752045945926, 0.7105605213783232, 0.6715332286760859, 0.5711899669416567, 0.5906940239606078, 0.6336532427680177, 0.7019053862803863, 0.7039181183386429, 0.7425065608962091, 0.6373853163721832, 0.6914936025140285, 0.6018149676410546, 0.6982748182665968, 0.6321593584986441, 0.6190456486912125, 0.712022438960148], 'Vehicle': [0.6423634198272342, 0.6315244820232104, 0.6339857088658776, 0.6307312827006775, 0.6388461250714235, 0.6350738172764704, 0.623445015529513, 0.6262108675594227, 0.6341679263789928, 0.6247094081816671], 'Waveform3': [0.7197862698301906, 0.7063972548193982, 0.7400643492141958, 0.7126972429257097, 0.7025721625371153, 0.7337472498868374, 0.7249646497641623, 0.7333692293485765, 0.7182918470642754, 0.7153980018069834, 0.7348361650445674, 0.6890851342835564, 0.6923553583789588, 0.7163152535340829, 0.7068985654728488, 0.7253250963782822, 0.7316709575809863, 0.7191820887618579, 0.7171802675374419, 0.7389326602827241, 0.7021049073211434, 0.7018873612755687, 0.6893525767501423, 0.7211942056622049, 0.7162441108435126, 0.6846067792489656, 0.7047986552263523, 0.7020730515320308, 0.6913580836035378, 0.6813327145723158], 'Winequality': [0.578896395705878, 0.41007262774515324, 0.5576556170079551, 0.5470141489357672, 0.4013411063320317, 0.4132692007089447, 0.41283157764085227, 0.42219614867375727, 0.41932570992146123, 0.5821194408136688], 'SVMguide1': [0.913260836131434, 0.9143473478965896, 0.8056844810151904, 0.9121464199102003, 0.8966598344036748, 0.909872720975909, 0.909915209361302, 0.9133521481927394, 0.9077920176054599, 0.9042478483820853, 0.9032293057399599, 0.8101998199112617, 0.9052416017439614, 0.7959575404379484, 0.9115018299293214, 0.9165181863664982, 0.9094256692129954, 0.9086624448493879, 0.9150923349630278, 0.9067551471406239], 'Diabetes': [0.5046082031196657, 0.5417564928658983, 0.4658649822537901, 0.5317265964825562, 0.5142110147868257, 0.46547413429517914, 0.5238813550721679, 0.49782638804532586, 0.5360835741108734, 0.4542502305055387], 'Vowel': [0.8241169826176831, 0.836079215015703, 0.8332167983015967, 0.8414047163671488, 0.842336224210134, 0.8368305153657072, 0.8276267116055624, 0.8357441439644513, 0.8460712927525866, 0.8349704246463479]}\n"
     ]
    }
   ],
   "source": [
    "print('f1_micro : ',f1_micro ,' f1_macro : ', f1_macro, 'f1_weighted : ', f1_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'tf'",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
